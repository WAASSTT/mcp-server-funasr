#!/usr/bin/env python3
"""é¢„ä¸‹è½½ FunASR å’Œ LLM æ¨¡å‹ v4.0.0

æ”¯æŒçš„æ¨¡å‹:
ã€è¯­éŸ³è¯†åˆ«æ¨¡å‹ - å¿…éœ€ã€‘
1. paraformer-zh-streaming - å®æ—¶æµå¼è¯†åˆ«æ¨¡å‹ (Paraformer-online)
2. paraformer-zh - æ‰¹é‡è¯­éŸ³è¯†åˆ«æ¨¡å‹ (Paraformer-large)
3. fsmn-vad - è¯­éŸ³æ´»åŠ¨æ£€æµ‹æ¨¡å‹

ã€å¢å¼ºæ¨¡å‹ - æ‰¹é‡è¯†åˆ«å¿…éœ€ã€‘
4. ct-punc - æ ‡ç‚¹ç¬¦å·æ¢å¤æ¨¡å‹ (CT-Transformer, æ‰¹é‡è¯†åˆ«ç”¨)
5. cam++ - è¯´è¯äººåˆ†ç¦»æ¨¡å‹ (CAM++, æ‰¹é‡è¯†åˆ«ç”¨)

ã€LLM åå¤„ç†æ¨¡å‹ - GGUF æ ¼å¼ - å®æ—¶è¯†åˆ«ç”¨ã€‘
6. Qwen2.5-7B-Instruct-GGUF - æµå¼ LLM åå¤„ç†ï¼ˆæ¨èï¼Œ~4.5GBï¼‰
7. Qwen2.5-1.8B-Instruct-GGUF - è½»é‡çº§ LLMï¼ˆä½é…æ¨èï¼Œ~1.5GBï¼‰

ç‰ˆæœ¬: 4.0.0
æ›´æ–°æ—¥æœŸ: 2025-12-23
"""

import os
import sys

os.environ["MODELSCOPE_CACHE"] = "./Model"

try:
    from funasr import AutoModel
    from modelscope.hub.api import HubApi
except ImportError:
    print("âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ä¾èµ–")
    print("è¯·å…ˆå®‰è£…ä¾èµ–: uv pip install funasr modelscope")
    sys.exit(1)


def download_model(model_id: str, model_name: str, device: str = "cpu"):
    """ä¸‹è½½ASRæ¨¡å‹"""
    print(f"\n{'='*60}\nğŸ“¦ æ­£åœ¨ä¸‹è½½: {model_name}\n   æ¨¡å‹ID: {model_id}\n{'='*60}")
    try:
        model = AutoModel(model=model_id, device=device, disable_update=True, model_hub="ms")
        print(f"âœ… {model_name} ä¸‹è½½æˆåŠŸ!")
        return True
    except Exception as e:
        print(f"âŒ {model_name} ä¸‹è½½å¤±è´¥: {e}")
        return False


def download_pipeline_model(model_id: str, model_name: str, task: str):
    """ä¸‹è½½Pipelineæ¨¡å‹ï¼ˆå¦‚è¯­éŸ³å¢å¼ºï¼‰"""
    print(f"\n{'='*60}\nğŸ“¦ æ­£åœ¨ä¸‹è½½: {model_name}\n   æ¨¡å‹ID: {model_id}\n{'='*60}")
    try:
        from modelscope.pipelines import pipeline
        from modelscope.utils.constant import Tasks
        pipe = pipeline(task=getattr(Tasks, task, task), model=model_id)
        print(f"âœ… {model_name} ä¸‹è½½æˆåŠŸ!")
        return True
    except Exception as e:
        print(f"âŒ {model_name} ä¸‹è½½å¤±è´¥: {e}")
        return False


def download_gguf_model(repo_id: str, filename: str, model_name: str):
    """ä¸‹è½½GGUFæ ¼å¼çš„LLMæ¨¡å‹ï¼ˆä»HuggingFaceï¼‰"""
    print(f"\n{'='*60}\nğŸ“¦ æ­£åœ¨ä¸‹è½½: {model_name}\n   ä»“åº“: {repo_id}\n   æ–‡ä»¶: {filename}\n{'='*60}")
    try:
        from huggingface_hub import hf_hub_download
        save_dir = os.path.join("./Model/models/Qwen")
        os.makedirs(save_dir, exist_ok=True)
        file_path = hf_hub_download(repo_id=repo_id, filename=filename, local_dir=save_dir)
        print(f"âœ… {model_name} ä¸‹è½½æˆåŠŸ!\n   ä¿å­˜è·¯å¾„: {file_path}")
        return True
    except Exception as e:
        print(f"âŒ {model_name} ä¸‹è½½å¤±è´¥: {e}\nğŸ’¡ æ‰‹åŠ¨ä¸‹è½½: https://huggingface.co/{repo_id}/tree/main")
        if "huggingface_hub" in str(e):
            print("   è¯·å®‰è£…: pip install huggingface-hub")
        return False


def check_existing_models():
    """æ£€æŸ¥å·²ä¸‹è½½çš„æ¨¡å‹"""
    print("\n" + "=" * 60 + "\nğŸ” æ£€æŸ¥ç°æœ‰æ¨¡å‹...\n" + "=" * 60)
    model_dir = "./Model"
    os.makedirs(model_dir, exist_ok=True)

    existing = []
    for root, dirs, files in os.walk(model_dir):
        if "model.pt" in files or "config.yaml" in files or any(".gguf" in f for f in files):
            existing.append(os.path.relpath(root, model_dir))

    if existing:
        for e in existing:
            print(f"  âœ“ {e}")
    else:
        print("  æœªæ‰¾åˆ°å·²ä¸‹è½½çš„æ¨¡å‹")
    return existing


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60 + "\nFunASR æ¨¡å‹ä¸‹è½½å·¥å…· v3.0.0\n" + "=" * 60)
    print(f"ç¼“å­˜ç›®å½•: {os.path.abspath('./Model')}")
    check_existing_models()

    # å®šä¹‰æ‰€æœ‰æ¨¡å‹
    core_models = [
        {"id": "paraformer-zh-streaming", "name": "Paraformer-Streaming (å®æ—¶)", "type": "asr"},
        {"id": "paraformer-zh", "name": "Paraformer-zh (æ‰¹é‡)", "type": "asr"},
        {"id": "fsmn-vad", "name": "FSMN-VAD (VAD)", "type": "asr"},
    ]
    batch_models = [
        {"id": "ct-punc-c", "name": "CT-Punc (æ ‡ç‚¹-æ‰¹é‡å¿…éœ€)", "type": "asr"},
        {"id": "cam++", "name": "CAM++ (è¯´è¯äºº-æ‰¹é‡å¿…éœ€)", "type": "asr"},
    ]
    gguf_models = [
        {"repo_id": "Qwen/Qwen2.5-7B-Instruct-GGUF", "filename": "qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf", "name": "Qwen2.5-7B-Q4_K_M (part 1/2)", "type": "gguf"},
        {"repo_id": "Qwen/Qwen2.5-7B-Instruct-GGUF", "filename": "qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf", "name": "Qwen2.5-7B-Q4_K_M (part 2/2)", "type": "gguf"},
        {"repo_id": "Qwen/Qwen2.5-7B-Instruct-GGUF", "filename": "qwen2.5-7b-instruct-q3_k_m.gguf", "name": "Qwen2.5-7B-Q3_K_M (è½»é‡)", "type": "gguf"},
    ]

    print(f"\nå¯ç”¨æ¨¡å‹: {len(core_models)}æ ¸å¿ƒ + {len(batch_models)}æ‰¹é‡ + {len(gguf_models)}LLM")
    print("\nè¯·é€‰æ‹©:")
    print("  1. æ ¸å¿ƒæ¨¡å‹ (æœ€å°-ä»…å®æ—¶è¯†åˆ«)")
    print("  2. æ ¸å¿ƒ+æ‰¹é‡æ¨¡å‹ (å…¨åŠŸèƒ½)")
    print("  3. æ ¸å¿ƒ+Qwen2.5-7B-GGUF (å®æ—¶+LLMä¼˜åŒ–)")
    print("  4. å®Œæ•´å®‰è£… (å…¨éƒ¨-æ¨è)")
    print("  5. è‡ªå®šä¹‰")
    print("  6. é€€å‡º")

    choice = input("\nè¯·è¾“å…¥ [1-6]: ").strip()
    models_to_download = []

    if choice == "1": models_to_download = core_models
    elif choice == "2": models_to_download = core_models + batch_models
    elif choice == "3": models_to_download = core_models + gguf_models[:2]  # part1 + part2
    elif choice == "4": models_to_download = core_models + batch_models + gguf_models[:2]
    elif choice == "5":
        models_to_download = core_models.copy()
        print("\næ‰¹é‡è¯†åˆ«æ¨¡å‹ (å¿…é€‰ä¸€èµ·):")
        add_batch = input("  æ·»åŠ æ‰¹é‡æ¨¡å‹? (y/n): ").strip().lower() == 'y'
        if add_batch: models_to_download.extend(batch_models)
        print("\nLLMæ¨¡å‹:")
        for i, m in enumerate(gguf_models): print(f"  {i+1}. {m['name']}")
        llm = input("é€‰æ‹©LLM (åºå·,ç•™ç©ºè·³è¿‡): ").strip()
        if llm.isdigit() and 0 <= int(llm)-1 < len(gguf_models):
            idx = int(llm)-1
            if idx <= 1:  # å¦‚æœé€‰part1æˆ–part2ï¼Œéƒ½ä¸‹è½½
                models_to_download.extend(gguf_models[:2])
            else:
                models_to_download.append(gguf_models[idx])
    elif choice == "6": return
    else:
        print("æ— æ•ˆé€‰é¡¹")
        return

    if not models_to_download:
        print("æœªé€‰æ‹©æ¨¡å‹")
        return

    # ä¸‹è½½
    success = 0
    print(f"\n{'='*60}\nå¼€å§‹ä¸‹è½½ {len(models_to_download)} ä¸ªæ¨¡å‹\n{'='*60}")
    for m in models_to_download:
        t = m.get("type", "asr")
        if t == "gguf":
            r = download_gguf_model(m["repo_id"], m["filename"], m["name"])
        else:
            r = download_model(m["id"], m["name"])
        if r: success += 1

    print(f"\n{'='*60}\nä¸‹è½½å®Œæˆ! âœ… {success}/{len(models_to_download)}\n{'='*60}")
    if success > 0:
        print("\nğŸ’¡ æç¤º:")
        print("  - æ¨¡å‹ä¿å­˜åœ¨: ./Model/")
        print("  - è¿è¡ŒæœåŠ¡å™¨: python main.py")
        if any(m.get("type")=="gguf" for m in models_to_download):
            print("\nğŸš€ LLMé…ç½®:")
            print("  pip install llama-cpp-python")
            print("  enable_llm_postprocess=True")
            print("  llm_model_path='./Model/models/Qwen/*.gguf'")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
